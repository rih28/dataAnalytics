{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part-4-4.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhwzuMjrRdpGd/ble1L/TW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rih28/dataAnalytics/blob/master/Part_4_4_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xDzW5OBl-pm",
        "colab_type": "text"
      },
      "source": [
        "This problem is slightly different from the other example shown. We have the training and test sets separated into two data sets. These data sets are actually different from the ones used in the example we have been following, but the concepts are the same and are still for the california housing problem. \n",
        "\n",
        "First thing is to obtain the data using pandas. Again, notice the use of two separate variables, df_train and df_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12m-ew9QzhQf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "b946e1cd-299d-47b3-b5fa-6cfcd4a40ff5"
      },
      "source": [
        "# needed to create the data frame\n",
        "import pandas as pd\n",
        "\n",
        "# create data frame from csv file we hosted on our github\n",
        "df_train = pd.read_csv('https://raw.githubusercontent.com/rih28/dataAnalytics/master/FinalTS.csv', index_col=0)\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/rih28/dataAnalytics/master/FinalTST.csv', index_col=0)\n",
        "\n",
        "print(df_train[:6])\n",
        "print(df_test[:6])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   longitude  latitude  ...  population_per_household  median_house_value\n",
            "1    -122.23     37.88  ...                  1.819209              452600\n",
            "2    -122.22     37.86  ...                  3.362745              358500\n",
            "4    -122.25     37.85  ...                  1.388060              341300\n",
            "5    -122.25     37.85  ...                  1.207265              342200\n",
            "6    -122.25     37.85  ...                  1.501818              269700\n",
            "7    -122.25     37.84  ...                  2.755668              299200\n",
            "\n",
            "[6 rows x 13 columns]\n",
            "    longitude  latitude  ...  population_per_household  median_house_value\n",
            "3     -122.24     37.85  ...                  2.802260              352100\n",
            "10    -122.25     37.84  ...                  2.172269              261100\n",
            "11    -122.26     37.85  ...                  2.263682              281500\n",
            "13    -122.26     37.85  ...                  2.346154              213500\n",
            "20    -122.27     37.84  ...                  2.509091              162900\n",
            "28    -122.28     37.85  ...                  2.775819              105500\n",
            "\n",
            "[6 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl_4vZi0moa2",
        "colab_type": "text"
      },
      "source": [
        "Next we use numpy to get the first 11 (apart from the id's in column 1) as our predictors, and leaving the final column as our target values. We need to do this for both the training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpdzgF1T0Yg3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "0d8ee9b4-00ab-4a6e-fd0a-d2884fda871a"
      },
      "source": [
        "# needed to help with speedy maths based calculations\n",
        "import numpy as np\n",
        "\n",
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "df_train = df_train.iloc[np.random.permutation(len(df_train))]\n",
        "\n",
        "predictors_train = df_train.iloc[:,1:12]\n",
        "predictors_test = df_test.iloc[:,1:12]\n",
        "print(predictors_test)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       latitude  ...  population_per_household\n",
            "3         37.85  ...                  2.802260\n",
            "10        37.84  ...                  2.172269\n",
            "11        37.85  ...                  2.263682\n",
            "13        37.85  ...                  2.346154\n",
            "20        37.84  ...                  2.509091\n",
            "...         ...  ...                       ...\n",
            "20614     39.09  ...                  3.039062\n",
            "20615     39.08  ...                  3.069620\n",
            "20617     39.08  ...                  3.085333\n",
            "20622     39.01  ...                  3.082803\n",
            "20630     39.12  ...                  3.801980\n",
            "\n",
            "[4126 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2jpwZBVm8J2",
        "colab_type": "text"
      },
      "source": [
        "Similarly, for the targets i.e. the median_house_value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-1na9ag1KSH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "e6b9e3f0-2068-4bce-9f6a-99048944fd05"
      },
      "source": [
        "targets_train = df_train.iloc[:,12:13]\n",
        "targets_test = df_test.iloc[:,12:13]\n",
        "print(targets_train)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       median_house_value\n",
            "5104               120000\n",
            "17992              234300\n",
            "14968              153800\n",
            "5654               268500\n",
            "4720               500000\n",
            "...                   ...\n",
            "934                211300\n",
            "10354              202200\n",
            "9562               110200\n",
            "19342              160000\n",
            "1239               103800\n",
            "\n",
            "[16514 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwq2qirXnA3K",
        "colab_type": "text"
      },
      "source": [
        "We use a SCALE value of 1000000 due to the size of values in the median_house_value. This is to help the model train better as nodes train better with values between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcpOmMBB2vnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SCALE = 1000000.0 # Need this to scale the results."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KiR5fJrnfTz",
        "colab_type": "text"
      },
      "source": [
        "Get the size of the training and test datasets. Here, we use a single column to measure the length. It doesn't matter which column you use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NujWMnQq248L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainsize = int(len(df_train['median_house_value']))\n",
        "testsize = int(len(df_test['median_house_value']))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvurbcTlniun",
        "colab_type": "text"
      },
      "source": [
        "Get the number of input values i.e. nppredictors which is 11. Also, we set the number of outputs. We only want 1, a prediction of the median_house_value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cIXkkia3Fpe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b99e4fda-9c79-4d18-e3b4-0472c2d255ae"
      },
      "source": [
        "nppredictors = len(predictors_train.columns);\n",
        "print(len(predictors_test.columns))\n",
        "noutputs = 1;"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZieNJFZUn5YC",
        "colab_type": "text"
      },
      "source": [
        "Set up tensorflow for training a DNN Regressor. The only part that is different in the setup from the Linear Regressor (apart from the name) is the hidden_units.\n",
        "\n",
        "This set's up a deep neural network with an input of 11, hidden layer 1 has 20 nodes, hidden layer 2 has 18 and hidden layer has 12 nodes with an output of 1. You will read more about this configuration soon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajqj7Vmc3Qts",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df5b7e11-9b56-4b8b-ad1e-135d6630324d"
      },
      "source": [
        "# import tensorflow\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# check the version\n",
        "print(tf.__version__)\n",
        "\n",
        "# needed for high-level file management\n",
        "import shutil  \n",
        "\n",
        "# logging for tensorflow\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "# removes a saved model from the last training attempt.\n",
        "shutil.rmtree('/tmp/DNN_house_regression_trained_model', ignore_errors=True)\n",
        "\n",
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_house_regression_trained_model', hidden_units=[20,18,14], optimizer=tf.train.AdamOptimizer(learning_rate=0.01), enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors_train.values)))\n",
        "\n",
        "# Prints a log to show model is starting to train\n",
        "print(\"starting to train\");\n",
        "\n",
        "# Train the model. Pass in predictor values and target values.\n",
        "estimator.fit(predictors_train.values, targets_train.values.reshape(trainsize, noutputs)/SCALE, steps=10000)\n",
        "\n",
        "# Next, we can check our predictions based on our predictors.\n",
        "preds = estimator.predict(x=predictors_train.values)\n",
        "\n",
        "# Apply the Scale value (not really needed here) to the outputs.\n",
        "predslistscale = preds['scores']*SCALE\n",
        "\n",
        "# pred = format(str(predslistscale)) # useful for checking outputs and printing.\n",
        "\n",
        "# Calculate RMSE i.e. how good the model works using the predictions and targets.\n",
        "# i.e. take the difference between the actual and the forecast then square the difference, \n",
        "# find the average of all the squares and then find the square root. \n",
        "# The RMSE essentially punishes larger errors i.e. it puts a heavier weight on larger errors.\n",
        "rmse = np.sqrt(np.mean((targets_train.values - predslistscale)**2))\n",
        "print('DNNRegression has RMSE of {0}'.format(rmse));\n",
        "\n",
        "\n",
        "# Calculate the mean of the Life Satisfaction Values.\n",
        "avg = np.mean(df_train['median_house_value'])\n",
        "\n",
        "# Calculate the RMSE using Life Satisfaction Values and the mean of all target values.\n",
        "# The fit of a proposed regression model should therefore be better than the fit of the mean model.\n",
        "# In this case, it doesn't seem to be the case but it will vary on every run.\n",
        "rmse = np.sqrt(np.mean((df_train['median_house_value'] - avg)**2))\n",
        "print('Just using average = {0} has RMSE of {1}'.format(avg, rmse));\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-7-35e82b418af6>:17: infer_real_valued_columns_from_input (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please specify feature columns explicitly.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:143: setup_train_data_feeder (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:96: extract_dask_data (from tensorflow.contrib.learn.python.learn.learn_io.dask_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please feed input to tf.data to support dask.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:100: extract_pandas_data (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please access pandas data directly.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:159: DataFeeder.__init__ (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:340: check_array (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please convert numpy dtypes explicitly.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:183: infer_real_valued_columns_from_input_fn (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please specify feature columns explicitly.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/estimators/dnn.py:660: regression_head (from tensorflow.contrib.learn.python.learn.estimators.head) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.contrib.estimator.*_head.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:1180: BaseEstimator.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:427: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb9a8abc18>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_house_regression_trained_model', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:From <ipython-input-7-35e82b418af6>:17: SKCompat.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to the Estimator interface.\n",
            "starting to train\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:98: extract_dask_labels (from tensorflow.contrib.learn.python.learn.learn_io.dask_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please feed input to tf.data to support dask.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:102: extract_pandas_labels (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please access pandas data directly.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/estimators/head.py:678: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/DNN_house_regression_trained_model/model.ckpt.\n",
            "INFO:tensorflow:loss = 23501.91, step = 1\n",
            "INFO:tensorflow:global_step/sec: 459.503\n",
            "INFO:tensorflow:loss = 4.017579, step = 101 (0.224 sec)\n",
            "INFO:tensorflow:global_step/sec: 570.667\n",
            "INFO:tensorflow:loss = 1.4446869, step = 201 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 613.259\n",
            "INFO:tensorflow:loss = 1.7805871, step = 301 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 561.843\n",
            "INFO:tensorflow:loss = 1.3864292, step = 401 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 615.811\n",
            "INFO:tensorflow:loss = 0.79674816, step = 501 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 571.364\n",
            "INFO:tensorflow:loss = 1.5692543, step = 601 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 565.308\n",
            "INFO:tensorflow:loss = 1.5919648, step = 701 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 596.864\n",
            "INFO:tensorflow:loss = 12.319195, step = 801 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 576.068\n",
            "INFO:tensorflow:loss = 0.2988264, step = 901 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 609.392\n",
            "INFO:tensorflow:loss = 0.4632098, step = 1001 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 581.85\n",
            "INFO:tensorflow:loss = 0.83118945, step = 1101 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 597.936\n",
            "INFO:tensorflow:loss = 0.5238303, step = 1201 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 573.347\n",
            "INFO:tensorflow:loss = 0.5650094, step = 1301 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 613.388\n",
            "INFO:tensorflow:loss = 0.16841851, step = 1401 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 584.736\n",
            "INFO:tensorflow:loss = 0.085399866, step = 1501 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 557.643\n",
            "INFO:tensorflow:loss = 0.45119455, step = 1601 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 611.283\n",
            "INFO:tensorflow:loss = 0.058789156, step = 1701 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 600.204\n",
            "INFO:tensorflow:loss = 0.40745726, step = 1801 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 606.753\n",
            "INFO:tensorflow:loss = 2.2782812, step = 1901 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 596.011\n",
            "INFO:tensorflow:loss = 0.04030232, step = 2001 (0.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 618.853\n",
            "INFO:tensorflow:loss = 0.107035495, step = 2101 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 581.483\n",
            "INFO:tensorflow:loss = 0.10210107, step = 2201 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 568.528\n",
            "INFO:tensorflow:loss = 0.04951445, step = 2301 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 630.183\n",
            "INFO:tensorflow:loss = 0.09966375, step = 2401 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 597.284\n",
            "INFO:tensorflow:loss = 0.01664846, step = 2501 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 560.867\n",
            "INFO:tensorflow:loss = 0.51768017, step = 2601 (0.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 598.79\n",
            "INFO:tensorflow:loss = 0.067305036, step = 2701 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 607.425\n",
            "INFO:tensorflow:loss = 0.04462619, step = 2801 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 612.549\n",
            "INFO:tensorflow:loss = 0.21617883, step = 2901 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 607.631\n",
            "INFO:tensorflow:loss = 0.08358745, step = 3001 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 588.526\n",
            "INFO:tensorflow:loss = 0.10739181, step = 3101 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 580.758\n",
            "INFO:tensorflow:loss = 0.022753142, step = 3201 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 620.702\n",
            "INFO:tensorflow:loss = 0.17556177, step = 3301 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 598.333\n",
            "INFO:tensorflow:loss = 1.6297464, step = 3401 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 516.184\n",
            "INFO:tensorflow:loss = 0.01101942, step = 3501 (0.197 sec)\n",
            "INFO:tensorflow:global_step/sec: 564.918\n",
            "INFO:tensorflow:loss = 34.98932, step = 3601 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 579.908\n",
            "INFO:tensorflow:loss = 0.043038078, step = 3701 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 597.653\n",
            "INFO:tensorflow:loss = 0.2874589, step = 3801 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 589.92\n",
            "INFO:tensorflow:loss = 0.140108, step = 3901 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 598.65\n",
            "INFO:tensorflow:loss = 0.0140969595, step = 4001 (0.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 611.184\n",
            "INFO:tensorflow:loss = 0.01587412, step = 4101 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 602.576\n",
            "INFO:tensorflow:loss = 0.015938073, step = 4201 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 590.414\n",
            "INFO:tensorflow:loss = 0.018600997, step = 4301 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 572.369\n",
            "INFO:tensorflow:loss = 0.019285137, step = 4401 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 577.959\n",
            "INFO:tensorflow:loss = 0.013973094, step = 4501 (0.167 sec)\n",
            "INFO:tensorflow:global_step/sec: 579.73\n",
            "INFO:tensorflow:loss = 0.040236358, step = 4601 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 586.168\n",
            "INFO:tensorflow:loss = 0.01018966, step = 4701 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 599.747\n",
            "INFO:tensorflow:loss = 0.013823863, step = 4801 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 541.528\n",
            "INFO:tensorflow:loss = 0.05586526, step = 4901 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 591.317\n",
            "INFO:tensorflow:loss = 0.009870901, step = 5001 (0.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 576.651\n",
            "INFO:tensorflow:loss = 0.018748598, step = 5101 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 589.28\n",
            "INFO:tensorflow:loss = 0.011196836, step = 5201 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 600.799\n",
            "INFO:tensorflow:loss = 0.013158517, step = 5301 (0.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 598.256\n",
            "INFO:tensorflow:loss = 0.01405617, step = 5401 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 571.828\n",
            "INFO:tensorflow:loss = 0.008037744, step = 5501 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 585.73\n",
            "INFO:tensorflow:loss = 0.022053057, step = 5601 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 568.078\n",
            "INFO:tensorflow:loss = 0.009777208, step = 5701 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 572.985\n",
            "INFO:tensorflow:loss = 0.0078037335, step = 5801 (0.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 584.494\n",
            "INFO:tensorflow:loss = 0.019579092, step = 5901 (0.173 sec)\n",
            "INFO:tensorflow:global_step/sec: 604.934\n",
            "INFO:tensorflow:loss = 0.008904056, step = 6001 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 601.751\n",
            "INFO:tensorflow:loss = 0.009218104, step = 6101 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 620.118\n",
            "INFO:tensorflow:loss = 0.01076125, step = 6201 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 609.36\n",
            "INFO:tensorflow:loss = 0.02046488, step = 6301 (0.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 628.626\n",
            "INFO:tensorflow:loss = 0.08662142, step = 6401 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 640.233\n",
            "INFO:tensorflow:loss = 0.015487057, step = 6501 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 616.274\n",
            "INFO:tensorflow:loss = 0.008084148, step = 6601 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 631.738\n",
            "INFO:tensorflow:loss = 0.92638725, step = 6701 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 619.892\n",
            "INFO:tensorflow:loss = 0.025937648, step = 6801 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 584.694\n",
            "INFO:tensorflow:loss = 0.011726657, step = 6901 (0.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 639.232\n",
            "INFO:tensorflow:loss = 0.0100315735, step = 7001 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 614.686\n",
            "INFO:tensorflow:loss = 0.09352223, step = 7101 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 618.648\n",
            "INFO:tensorflow:loss = 0.011141558, step = 7201 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 631.192\n",
            "INFO:tensorflow:loss = 0.011170272, step = 7301 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 615.553\n",
            "INFO:tensorflow:loss = 0.013652694, step = 7401 (0.162 sec)\n",
            "INFO:tensorflow:global_step/sec: 629.243\n",
            "INFO:tensorflow:loss = 0.014159796, step = 7501 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 594.979\n",
            "INFO:tensorflow:loss = 0.011678681, step = 7601 (0.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 658.659\n",
            "INFO:tensorflow:loss = 0.010851316, step = 7701 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 632.57\n",
            "INFO:tensorflow:loss = 0.011827873, step = 7801 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 612.187\n",
            "INFO:tensorflow:loss = 0.04389202, step = 7901 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 620.949\n",
            "INFO:tensorflow:loss = 0.008635057, step = 8001 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 627.386\n",
            "INFO:tensorflow:loss = 0.009549212, step = 8101 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 634.258\n",
            "INFO:tensorflow:loss = 0.013845215, step = 8201 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 643.976\n",
            "INFO:tensorflow:loss = 0.05360928, step = 8301 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 633.62\n",
            "INFO:tensorflow:loss = 0.01206431, step = 8401 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 640.969\n",
            "INFO:tensorflow:loss = 0.0103165805, step = 8501 (0.157 sec)\n",
            "INFO:tensorflow:global_step/sec: 591.385\n",
            "INFO:tensorflow:loss = 0.008662927, step = 8601 (0.166 sec)\n",
            "INFO:tensorflow:global_step/sec: 639.205\n",
            "INFO:tensorflow:loss = 0.011193568, step = 8701 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 647.623\n",
            "INFO:tensorflow:loss = 0.007866146, step = 8801 (0.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 621.963\n",
            "INFO:tensorflow:loss = 0.013145968, step = 8901 (0.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 664.096\n",
            "INFO:tensorflow:loss = 0.009701286, step = 9001 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 656.043\n",
            "INFO:tensorflow:loss = 0.011502072, step = 9101 (0.155 sec)\n",
            "INFO:tensorflow:global_step/sec: 632.165\n",
            "INFO:tensorflow:loss = 0.011547944, step = 9201 (0.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 591.132\n",
            "INFO:tensorflow:loss = 0.009355487, step = 9301 (0.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 644.227\n",
            "INFO:tensorflow:loss = 0.011483562, step = 9401 (0.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 617.796\n",
            "INFO:tensorflow:loss = 0.010532634, step = 9501 (0.159 sec)\n",
            "INFO:tensorflow:global_step/sec: 623.456\n",
            "INFO:tensorflow:loss = 0.0085118795, step = 9601 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 627.18\n",
            "INFO:tensorflow:loss = 0.010391787, step = 9701 (0.160 sec)\n",
            "INFO:tensorflow:global_step/sec: 550.722\n",
            "INFO:tensorflow:loss = 0.008950759, step = 9801 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 595.828\n",
            "INFO:tensorflow:loss = 0.0076652337, step = 9901 (0.168 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/DNN_house_regression_trained_model/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0092776865.\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_house_regression_trained_model/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "DNNRegression has RMSE of 125316.80740538635\n",
            "Just using average = 206766.87453070123 has RMSE of 115180.61192472257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgrG2rSFoimo",
        "colab_type": "text"
      },
      "source": [
        "Same problem applies with training and overfitting, so just re-run cells if values aren't reasonable. However, this is mostly just an exercise and is not really required. You can see we run the model differently here with no user constructed input data frame. The set-up is the same as the training part above, but there is no need for the optimizer. \n",
        "\n",
        "You can see we insert the predictors_test.values, which are the test set input values.\n",
        "\n",
        "The overall output is two arrays (that only shows a few outputs. The predicted values seem reasonable and are not just the same, so the model has learned reasonably well but they seem to differ quite a bit from the target values (actual outputs).\n",
        "\n",
        "predicted [239933.05 247445.73 215407.52 ... 149025.11 162564.19 160469.2 ]\n",
        "targets   [352100    261100    281500    ... 55100     77500     108300   ]\n",
        "\n",
        "Remember, this is real data and very noisy. But the data isn't quite as bad as it seems. You can see that higher predicted values match the higher targets. The numbers aren't exactly correct but could certainly be used to make rough estimates i.e. these values give this median_house_value which is low/medium/high in scale. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwmIaI6_5FYc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "ab0e4c59-4073-4072-e479-68a0bd9735eb"
      },
      "source": [
        "estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(model_dir='/tmp/DNN_house_regression_trained_model', hidden_units=[20,18,14], enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors_test.values)))\n",
        "\n",
        "preds = estimator.predict(x=predictors_test.values)\n",
        "predslistscale = preds['scores']*SCALE\n",
        "pred = format(str(predslistscale))\n",
        "#np.set_printoptions(threshold='nan')\n",
        "print(\"predicted\", pred)\n",
        "print(\"targets\", targets_test['median_house_value'].values)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb96fbda20>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DNN_house_regression_trained_model', '_session_creation_timeout_secs': 7200}\n",
            "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/DNN_house_regression_trained_model/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "predicted [239933.05 247445.73 215407.52 ... 149025.11 162564.19 160469.2 ]\n",
            "targets [352100 261100 281500 ...  55100  77500 108300]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEgHrEbVrxaT",
        "colab_type": "text"
      },
      "source": [
        "**Go back to the course text**"
      ]
    }
  ]
}